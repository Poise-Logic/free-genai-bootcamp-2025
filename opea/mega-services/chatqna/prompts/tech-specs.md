*Generated by `Gemini 2.5 Pro Preview 03-25` using personal notes*

**Note:** Please refer to the readme file for an up-to-date guide specific to this deployment.

----
# Technical Specification: OPEA ChatQnA MegaService Deployment on AIPC (Single Node, Ollama)

## 1. Overview

This document specifies the technical requirements for deploying the OPEA ChatQnA MegaService on a single AIPC or Intel CPU node using Docker Compose. The deployment utilizes OPEA GenAIComps microservices, Ollama for the LLM, TEI for embeddings, and Redis as the vector database. It targets an on-premises environment running Docker Desktop on Windows 11 with a WSL2 (Ubuntu 24.04) backend.

## 2. System Architecture

*   **Deployment Model:** Single-node, on-premises via Docker Compose.
*   **Orchestration:** Docker Compose.
*   **Core Pipeline:** Data Prep -> Embedding (TEI) -> Retriever -> LLM (Ollama) -> Backend -> UI.
*   **Supporting Services:** Redis (Vector DB), Nginx (Reverse Proxy).
*   **Target Environment:** Windows 11 + Docker Desktop + WSL2 (Ubuntu 24.04).

## 3. Core Components & Configuration

| Service Name                  | Docker Image                                           | Key Config / Model                     | Ports (Internal:External) | Volumes        | Depends On                                                           |
| :---------------------------- | :----------------------------------------------------- | :------------------------------------- | :------------------------ | :------------- | :------------------------------------------------------------------- |
| `redis-vector-db`             | `redis/redis-stack:7.2.0-v9`                           | Vector Search Module                 | `6379:6379`               | `redis_data`   | -                                                                    |
| `dataprep`                    | `${REGISTRY:-opea}/dataprep:${TAG:-latest}`            | Uses `REDIS_URL`                      | `6007:6007`               | -              | `redis-vector-db`                                                    |
| `tei-embedding`               | `ghcr.io/huggingface/text-embeddings-inference:cpu-1.6` | Model: `BAAI/bge-base-en-v1.5`        | `80:6006`                 | -              | -                                                                    |
| `retriever`                   | `${REGISTRY:-opea}/retriever:${TAG:-latest}`           | Uses `REDIS_URL`, `TEI_EMBEDDING_ENDPOINT` | `7000:7000`               | -              | `redis-vector-db`, `tei-embedding`                                     |
| `ollama`                      | `ollama/ollama`                                        | Model: `llama3.2` (Pull post-start)   | `11434:11434`             | `ollama_data`  | -                                                                    |
| `chatqna-aipc-backend-server` | `${REGISTRY:-opea}/chatqna:${TAG:-latest}`             | Uses `set_env.sh` vars                 | `8888:8888`               | -              | `dataprep`, `tei-embedding`, `retriever`, `ollama`, `redis-vector-db` |
| `chatqna-aipc-ui-server`      | `${REGISTRY:-opea}/chatqna-ui:${TAG:-latest}`          | `BACKEND_URL=http://chatqna-aipc-backend-server:8888` | `5173:5173` | -              | `chatqna-aipc-backend-server`                                        |
| `nginx`                       | `${REGISTRY:-opea}/nginx:${TAG:-latest}`               | Reverse Proxy                          | `80:8000`                 | -              | `chatqna-aipc-ui-server`, `chatqna-aipc-backend-server`              |

*   **Volumes:** `redis_data`, `ollama_data` must be defined as named volumes.
*   **Ollama Model:** The `llama3.2` model must be pulled *after* the `ollama` container starts using `docker exec <container_id> ollama pull llama3.2`.

## 4. Configuration Files

*   **`set_env.sh`:**
    *   **Location:** Project root (`/mnt/c/Users/<User>/opea/mega-services/chatqna/` context).
    *   **Permissions:** Executable (`chmod +x`).
    *   **Content:** Bash script defining required environment variables:
        *   `HUGGINGFACEHUB_API_TOKEN="PLACE TOKEN HERE"` (User must replace).
        *   `REGISTRY="opea"`
        *   `TAG="latest"`
        *   `no_proxy="localhost,127.0.0.1"`
        *   `REDIS_URL="redis://redis-vector-db:6379"`
        *   `DATAPREP_ENDPOINT="http://dataprep:6007"`
        *   `TEI_EMBEDDING_ENDPOINT="http://tei-embedding:6006"`
        *   `EMBEDDING_MODEL_ID="BAAI/bge-base-en-v1.5"`
        *   `RETRIEVER_ENDPOINT="http://retriever:7000"`
        *   `OLLAMA_ENDPOINT="http://ollama:11434"`
        *   `OLLAMA_MODEL_NAME="llama3.2"`
        *   *(Include other required vars identified from reference)*
    *   **Exclusions:** No other proxy variables (`http_proxy`, `https_proxy`).
*   **`compose.yaml`:**
    *   **Location:** Project root.
    *   **Specification:** Docker Compose format (e.g., version 3.8).
    *   **Content:** Defines services as listed in Section 3, including image names (using `${REGISTRY:-opea}/${TAG:-latest}` pattern for OPEA images), ports, volumes, dependencies, and network settings.
    *   **Environment Loading:** Services requiring environment variables (e.g., `chatqna-aipc-backend-server`) must use `env_file: ./set_env.sh`.

## 5. Source Code & Image Management

*   **Required Repository:** `opea-project/GenAIComps` must be cloned into `~/OPEA` within WSL2.
*   **Image Acquisition (Primary):** Images will be pulled automatically by `docker compose up -d` or explicitly via `docker compose pull`.
*   **Image Acquisition (Optional - Build from Source):**
    *   Requires cloning `opea-project/GenAIComps` to `~/OPEA`.
    *   Build commands using `docker build -t <image_name>:<tag> -f <path_to_dockerfile> .` (run from appropriate base dir, typically `~/OPEA/GenAIComps`):
        *   `opea/dataprep:latest`: `comps/dataprep/src/Dockerfile`
        *   `opea/retriever:latest`: `comps/retrievers/langchain/docker/Dockerfile` (Verify path)
        *   `opea/nginx:latest`: `comps/nginx/Dockerfile`
        *   TEI Embedding: Refer to `comps/embeddings/README_tei.md`.
    *   Requires cloning `opea-project/GenAIExamples` to `~/OPEA` for optional builds:
        *   `opea/chatqna:latest`: `GenAIExamples/ChatQnA/Dockerfile` (Run from `~/OPEA`)
        *   `opea/chatqna-ui:latest`: `GenAIExamples/ChatQnA/ui/docker/Dockerfile` (Run from `~/OPEA`, context `GenAIExamples/ChatQnA/ui`)
    *   Optional Rerank Build: Refer to `comps/reranks/tei/Dockerfile` and `README_tei.md` (Hardware constraints apply).

## 6. Deployment Workflow (High-Level)

1.  **Setup:** Create project dir (Windows), create `~/OPEA` (WSL2).
2.  **Clone:** `git clone https://github.com/opea-project/GenAIComps.git ~/OPEA/GenAIComps`.
3.  **Configure:** Create `set_env.sh` (edit token) and `compose.yaml` in project dir.
4.  **Prepare Env:** `cd /path/to/project/dir && chmod +x set_env.sh`.
5.  **Source Env:** `source set_env.sh`.
6.  **Start Services:** `docker compose up -d`.
7.  **Pull LLM Model:** `docker exec <ollama_container_id> ollama pull llama3.2`.
8.  **Ingest Data:** Use Data Prep endpoint (`http://localhost:6007/v1/dataprep`).
9.  **Access UI:** `http://localhost:8000`.

## 7. Validation

*   **Container Status:** `docker compose ps` (All services running/healthy).
*   **Logs:** `docker compose logs <service_name>` (Check for errors).
*   **Environment:** `docker exec <container_id> printenv` (Verify variables).
*   **UI Access:** Browser connection to `http://localhost:8000`.
*   **Service Endpoints:** Basic checks using `curl` (e.g., `curl http://localhost:6006/info`).

## 8. Exclusions

*   The Reranking component is **not** part of this core deployment pipeline.
*   Complex proxy configurations (`http_proxy`, `https_proxy`) are not configured.

## 9. External References
*   [**OPEA GenAIExample - Building ChatQnA Megaservice on AIPC**](https://github.com/opea-project/GenAIExamples/tree/c48cd651e49d558ac6cfcd56324a021942ba9c25/ChatQnA/docker_compose/intel/cpu/aipc)
*   **OPEA Tutorial - ChatQnA on AIPC:** [https://opea-project.github.io/latest/tutorial/ChatQnA/deploy/aipc.html](https://opea-project.github.io/latest/tutorial/ChatQnA/deploy/aipc.htmlhttps://opea-project.github.io/latest/tutorial/ChatQnA/deploy/aipc.html)
*   **OPEA GenAIComps Repository:** [https://github.com/opea-project/GenAIComps](https://github.com/opea-project/GenAIComps)
*   **OPEA GenAIExamples Repository:** [https://github.com/opea-project/GenAIExamples](https://github.com/opea-project/GenAIExamples)
    *   Reference `compose.yaml`: `GenAIExamples/ChatQnA/docker_compose/intel/cpu/aipc/compose.yaml`
    *   Reference `set_env.sh`: `GenAIExamples/ChatQnA/docker_compose/intel/cpu/aipc/set_env.sh`
    *   Reference Logic: `GenAIExamples/ChatQnA/chatqna.py`
*   **TEI Embedding Instructions:** [https://github.com/opea-project/GenAIComps/blob/main/comps/embeddings/README_tei.md](https://github.com/opea-project/GenAIComps/blob/main/comps/embeddings/README_tei.md)
*   **Ollama:** [https://ollama.com/](https://ollama.com/)
*   **Redis Stack:** [https://redis.io/docs/stack/](https://redis.io/docs/stack/)